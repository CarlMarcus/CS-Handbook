

## 分布式系统原理

https://blog.csdn.net/sunyurun/article/details/8297419

分布式系统三态：任何请求都要考虑三种情况：**成功、失败、超时**。对于超时的请求，无法获知该请求是否被成功执行。

副本一致性分类：

> 强一致性：任何时刻任何用户/节点都可以读到最近一次更新成功的副本数据
> 单调一致性：任何时刻任何用户一旦读到某个数据某次更新后的值，就不会再读到更旧的值
> 会话一致性：任何时刻任何用户在某次会话内一旦读到某个数据某次更新后的值，就不会在这次会话再读到更旧的值
> 最终一致性：各个副本的数据最终将达到一致状态，但时间不保证
> 弱一致性：没有实用价值，略。

### 数据分布方式

基本问题：如何拆分分布式系统的数据。

1.哈希方式：元信息少，散列性号，扩展性差，容易数据倾斜。

2.按数据范围分布：扩展性强，但是元数据需要记录所有的数据分布情况，消耗大，维护困难

3.按数据量分布：与按范围分布数据方式类似，元信息容易成为瓶颈，但不会数据倾斜

4.一致性哈希

> 以机器为节点：随机分布节点的方式使得很难均匀的分布哈希值域， 一个节点异常时压力全转移到相邻节点，随机分布节点容易造成不均匀
>
> 增设虚节点：虚节点，虚节点个数远大于机器个数，将虚节点均匀分布到值域环上，通过元数据找到真实机器节点。某一个节点不可用导致多个虚节点不可用，均衡了压力，同理加入新节点导致增加多个虚节点，缓解了全局压力。

但是以上没考虑数据副本的问题。

如果以机器为副本，恢复效率低，可扩展性差，可用性也差。

把数据和副本均分段，然后哈希到不同机器上去，可以利用整个集群分担节点失效同步压力，可用性也高，缺点是需要维护的元信息比较多。

另外为了提高性能，避免网络带宽成为瓶颈，提出了“本地化计算”，数据在哪里，计算就在哪个节点进行。

**GFS和HDFS选择了按数据量分布，Cassandra倒是选择了一致性哈希。**

### 基本副本协议

副本怎么确保一定的可用性和一致性？两大类：中心化副本协议和去中心化副本协议。去中心化副本控制协议没有中心节点，节点之间通过平等协商达到一致。工程中唯一能应用在强一致性要求下的是**paxos协议**。后续介绍。

中心化副本协议：由一个中心节点协调副本数据的更新、维护副本之间一致性，并发控制

常用模型：primary-secondary协议

更新：primary副本接受更新请求，决定更新操作先后顺序然后将更新操作发给secondary节点，primary根据secondary回复的情况决定此次更新是否成功。

读取：

方法一：由于数据更新流程都是由primary控制的，primary副本上数据一定最新。所以永远只读primary副本的数据能够实现强一致性。为了避免机器浪费，可以使用之前讨论的方法，将数据分段，将数据段作为副本单位。达到每台机器都有primary副本的目的。

方法二：由primary控制secondary的可用性。当primary更新某个secondary不成功时，将其标记为不可用。不可用的secondary副本继续尝试与primary同步数据，直到成功同步后转为可用状态。

方法三：Quorum机制。后续讨论。

如何确定节点状态以发现原primary节点出现异常(Lease机制)。切换primary后不能影响一致性(Quorum机制)。

当发生secondary与primary不一致的情况，需要secondary向primary进行同步(reconcile)。

不一致的原因有3种：

1. 网络分化等异常导致secondary落后于primary，常用的方式是回放primary上的操作日志(redo log)，追上primary的更新进度

2. 某些协议下secondary是脏数据，丢弃转到第三种情况；或者设计基于undo log的方式

3. secondary是一个新增副本完全没有数据，直接copy primary的数据，但要求同时primary能继续提供更新服务，这就要求primary副本支持快照(snapshot)功能。即对某一刻的副本数据形成快照，然后copy快照，再使用回放日志的方式追赶快照后的更新操作。

### Lease机制

### Quorum机制

### 日志技术

### 两阶段提交协议



### MVCC分布式事务

### CAP理论

### Paxos协议