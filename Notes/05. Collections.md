## Collections

#### HashMap

> https://www.javadoop.com/post/hashmap

#### Hashmap

\1. 内部数据结构

Jdk7之前，entry<k,v>数组+链表、jdk8 Node<k,v>（方便转treenode）数组+链表/treeifybin()转红黑树

\2. 要点，细节

(1)     HashMap存储键值对，实现快速存取数据；允许null键/值，直接放下标为0的桶里；非线程安全；不保证有序(比如插入的顺序)。实现map接口

(2)     初始容量16，必须2的次方、方便定位的时候把取模转为位运算，最大容量1<<30,即2的30次方

(3)     默认加载因子 0.75，threshold = loadfactor*capacity

(4)     Jdk8，链表长度大于8且hashmap中size>64才转红黑树，size没到64就扩容

(5)     迭代器是fails-fast的

(6)     使用时设置初始值,避免多次扩容的性能消耗

(7)     使用自定义对象作为key时,需要重写hashCode和equals方法

\3. Hash定位过程

获取key的hashcode，做位运算、叫做哈希扰动、jdk7：4次位运算+5次异或 jdk8 高16位与低16位异或；其值与数组长-1做与运算，得到值就是下标。

不直接采用经过hashCode（）：哈希码 与 数组大小范围不匹配

为什么采用 哈希码 与运算(&) （数组长度-1） 计算数组下标：根据HashMap的容量大小（数组长度），按需取 哈希码一定数量的低位 作为存储的数组下标位置

\4. 扩容过程,resize 里面有rehash

简要：bucket扩充为2倍，之后重新计算index，把节点再transfer新的bucket中。当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置+oldcap，省去了重新计算hash值的时间。Capacity转二进制，0变1的那位，只要key的那位为0，位置就不变，key那位为1，就原位置+oldcap。

jdk7中resize，只有当 size>=threshold并且 table中的那个槽中已经有Entry时，才会发生resize。即有可能虽然size>=threshold，但是必须等到每个槽都至少有一个Entry时，才会扩容。

Transfer过程：对索引数组中的元素遍历；  对链表上的每一个节点遍历：用 next 取得要转移那个元素的下一个，将 e 转移到新 Hash 表的头部，使用头插法（jdk8改为尾插）插入节点； 遍历链表、遍历bucket

线程不安全：jdk7的头插法：转移数据、扩容后，容易出现链表逆序的情况，多线程下resize()容易出现死循环。此时若（多线程）并发执行 put（）操作，一旦出现扩容情况，则 容易出现 环形链表，从而在获取数据、遍历链表时 形成死循环。Jdk8改用尾插法，不会出此问题。

如果在使用迭代器的过程中有其他线程修改了map，那么将抛出ConcurrentModificationException，这就是所谓fail-fast策略。

\5. Put过程

根据hash得到的下标，看看bucket[i]里有没有entry，有开始遍历，若有重复key，更新，若无，头插或尾插，若无bucket[i]，则直接放入，jdk8会先判断一下size和链表长、看需不需要树化

\6. Remove：如果key对应的value存在，则删除这个键值对。 并返回value。如果不存在 返回null。

\7. Get过程：直接遍历bucket，有就返回，没有返回null。

\8. 与hashtable区别：hashtable synchronized线程安全，不允许键值null。Collections.synchronizedMap(new HashMap())可实现hashmap的线程安全。

\9. 使用String、Integer作为key的原因：1，不可变类、已有成熟的equals和hashcode重写、分部均匀还可以缓存hash、key不会变。重写equals需要保证key在hashmap中的唯一性、重写hashcode所需要保证其均匀性。

原生的equals问题在于判断的是两个对象的内存地址是否一样，hashcode问题在于是根据对象内存地址算出来的哈希码

 

 

#### ConcurrentHashMap

https://crossoverjie.top/2018/07/23/java-senior/ConcurrentHashMap/

https://zhuanlan.zhihu.com/p/35853397

https://cloud.tencent.com/developer/article/1350858

##### Jdk7

采用segment数组（默认16个，继承reentrantlock）和HashEntry（value和next是volatile的）组成。

**put**：通过 key 的hash运算定位到 Segment，之后在对应的 Segment 中进行具体的 put，虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。首先第一步的时候会**尝试获取锁**，如果获取失败肯定就有其他线程存在竞争，则利用 `scanAndLockForPut()` **自旋**获取锁，自旋一定次数后变为阻塞锁。获取到segment的锁后，将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry，遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。然后释放segment的锁。

**get：**只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。不需要加锁！

**扩容**：由于对segment加锁，所以获取到segment的锁后，内部可以保证是线程安全的，所以segment内部的扩容机制和jdk7的HashMap是一样的。

**Size()**是把各个segment中的entry个数累加，不过因为并发操作的特性，并不安全。先采用不加锁的方式，连续计算元素的个数，最多计算3次：

1、如果前后两次计算结果相同，则说明计算出来的元素个数是准确的；

2、如果前后两次计算结果都不同，则给每个Segment进行加锁，再计算一次元素的个数；

 

##### jdk8

不用segment，锁定粒度是bucket中的每一个槽位，把jdk7中的HashEntry改成了Node，采用了 `CAS + synchronized` 来保证并发安全性。

**put**：根据 key 计算出 hashcode ；Put如果相应位置的Node还未初始化（只有在执行第一次put方法时才会调用initTable()初始化Node数组），则通过CAS插入相应的数据；如果相应位置的Node不为空，且当前该节点不处于移动状态，则对该节点加synchronized锁；如果插入的是一个新节点，则执行addCount()方法尝试更新元素个数baseCount；如果这个位置显示正在进行扩容，就协助一起进行扩容

**get：**根据对象的key哈希获得Node位置，如果是红黑树那就按照树的方式获取值，否则按照链表的方式遍历获取值。也不需要加锁，因为volatile修饰的值，保证了内存可见性。

**扩容**：JDK8里面，去掉了分段锁，将锁的级别控制在了更细粒度的table元素级别，也就是说只需要锁住这个链表的head节点，并不会影响其他的table元素的读写，好处在于并发的粒度更细，影响更小，从而并发效率更好，但不足之处在于并发扩容的时候，由于操作的table都是同一个，不像JDK7中分段控制，所以这里需要等扩容完之后，所有的读写操作才能进行，所以扩容的效率就成为了整个并发的一个瓶颈点。好在Doug lea大神对扩容做了优化，本来在一个线程扩容的时候，如果影响了其他线程的数据，那么其他的线程的读写操作都应该阻塞，但Doug lea说你们闲着也是闲着，不如来一起参与扩容任务，这样人多力量大，办完事你们该干啥干啥，别浪费时间，于是在JDK8的源码里面就引入了一个ForwardingNode类，在一个线程发起扩容的时候，就会改变sizeCtl这个值，其含义如下：

```javascript
sizeCtl ：默认为0，用来控制table的初始化和扩容操作，具体应用在后续会体现出来。
-1 代表table正在初始化
-N 表示有N-1个线程正在进行扩容操作
其余情况：
1、如果table未初始化，表示table需要初始化的大小。
2、如果table初始化完成，表示table的容量，默认是table大小的0.75倍
```

扩容时候会判断这个值，如果超过阈值就要扩容，首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素f，初始化一个forwardNode实例fwd，如果f == null，则在table中的i位置放入fwd，否则采用头插法的方式把当前旧table数组的指定任务范围的数据给迁移到新的数组中，然后 给旧table原位置赋值fwd。直到遍历过所有的节点以后就完成了复制工作，把table指向nextTable，并更新sizeCtl为新数组大小的0.75倍 ，扩容完成。在此期间如果其他线程的有读写操作都会判断head节点是否为forwardNode节点，如果是就帮助扩容。

**Size实现**：1.8中使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据或则删除数据时，会通过CAS的addCount()方法更新baseCount。 

#### HashMap VS HashTable

HashTable的方法被synchronized修饰，不想允许key或value为null，当然ConcurrentHashMap也不允许，但HashMap允许、

#### ConcurrentHashMap VS HashTable

#### Fail-Fast VS Fail-Safe

**fail-fast**机制，是一种错误检测机制。  在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。JVM如果检测到可能会出现并发过程中的线程不安全问题就会抛出异常。它只能被用来做检测，因为JVM并不保证fail-fast抛出的异常一定会发生**。**若在多线程环境下使用fail-fast机制的集合，建议使用“java.util.concurrent包下的类”去取代“java.util包下的类”。

原理：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedModCount值，是的话就返回遍历；否则抛出异常，终止遍历。

采用fail-safe机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。java.util.concurrent包下的容器都是fail-safe的，可以在多线程下并发使用，并发修改。

#### foreach 循环里进行元素的 remove/add 操作

https://www.cnblogs.com/wucao/p/5350461.html

ConcurrentHashMap允许并行读取而不锁定，因为读取操作不需要锁定或线程安全。

ConcurrentHashMap 的高并发性主要来自于三个方面：

用分离锁实现多个线程间的更深层次的共享访问。

用 HashEntery 对象的不变性来降低执行读操作的线程在遍历链表期间对加锁的需求。

通过对同一个 Volatile 变量的写 / 读访问，协调不同线程间读 / 写操作的内存可见性。

#### 可以用HashTable替代ConcurrentHashMap吗？

HashTable虽然性能上不如ConcurrentHashMap，但并不能完全被取代，两者的迭代器的一致性不同的，HashTable的迭代器是强一致性的，而ConcurrentHashMap是弱一致的。 ConcurrentHashMap的get，clear，iterator 都是弱一致性的。

在构造迭代器 Iterator 时，通过 expectedModCount 记录了当前的修改次数；在通过 put 方法修改 Map 时，将 modCount （修改次数）域加1： ++modCount;在依次遍历迭代器 Iterator 时，判断 expectedModCount 是否与 modCount 相等：

当多钱程对集合进行结构上的改变或者集合在迭代元素时直接调用自身方法改变集合结构而没有通知迭代器时，有可能会触发fast-fail机制并抛出异常。

在使用iterator的同时，假设我们又去操作调用了ArrayList本身的方法比如ArrayList.remove(index),那么elementData数组的元素就会发生变化，而迭代器也要通过hasNext()方法判断，正好调用next方法，但是数据发生了变化而造成了数据不一致的问题。Fail-fast机制此时就会抛出ConcurrentModificationException。需要注意一点，有可能触发fast-fail机制而不是肯定。触发时机是在迭代过程中，集合的结构发生了变化而此时迭代器并不知道或者还没来得及反应时便会产生fail-fast机制。这里再次强调，迭代器的快速失败行为无法得到保证，因为一般来说，不可能对是否出现不同步并发修改或者自身修改做出任何硬性保证。

 

#### ArrayList

https://segmentfault.com/a/1190000016127895

使用ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity >> 1)，也就是旧容量的 1.5 倍。扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。删除需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)。

#### Vector

synchronized 进行同步，而且每次扩容请求其大小的 2 倍空间。

#### Linkedlist

继承自 HashMap，因此具有和 HashMap 一样的快速查找特性。内部维护了一个双向链表，用来维护插入顺序或者 LRU 顺序。